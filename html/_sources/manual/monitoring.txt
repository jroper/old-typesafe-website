Monitoring
==========

Overview
--------

The monitoring is highly influenced by Google's paper on `Dapper
<http://research.google.com/pubs/pub36356.html>`_ where they outline how they
have built their very scalable distributed systems tracing service.

|more| Read `this paper <http://research.google.com/pubs/pub36356.html>`_ for
some more detailed background.

In short it is attaching metadata to the messages as they flow through the
system, both across component boundaries as well as across the network. This
means that we can follow message chains in the distributed system and visualize
it in the Monitoring Dashboard through the concept of a ``span`` which
his latency for a message to move from A to B. We have many different types of
metadata and our tracked as the messages flow through the system. See the
configuration below for all the options for capturing statistics.

All this metadata is captured in a very efficient way using any combination of
thread local state and an immutable append-only tree data structure. There is
only one single contention point and that is when a full span is sent
(asynchronously) to the monitoring daemon. This is very different from most
statistics gathering products out there who are capturing statistics in either
atomic or volatile variables, this is something that is very expensive since
it's introducing hundreds of highly contended synchronization points in your
whole system.

The goal of Cloudy Akka Monitoring is to be so efficient that you should always
have it on in production.


Monitoring Daemons
------------------

It currently supports three different daemons:

  - ``LOCAL`` -- captures all statistics locally
  - ``REMOTE`` -- sends all statistics to a single monitoring server using TCP
  - ``FLUME`` -- sends all statistics to Flume which you can orchestrate to
    consolidate the metrics

Here is the configuration defining which daemons to run.

.. code-block:: conf

    akka {
      cloud {
        monitoring {
          daemons = ["LOCAL", "FLUME"] # Options: "LOCAL", "REMOTE", "FLUME"
        }
      }
    }

You boot up the local daemon(s) through the ``Monitoring`` object::

    import akka.cloud.monitoring._

    Monitoring.startLocalDaemons

    ...

    Monitoring.stopLocalDaemons

This way of starting the daemons will take the configuration from the
configuration file. You can however override that by explicitly starting up each
daemon::

    import akka.cloud.monitoring._

    Monitoring.startWithLocalDaemon

    Monitoring.startWithRemoteDaemon

    Monitoring.startWithFlumeDaemon

    ...

    Monitoring.stopLocalDaemons

The configuration also allows you to set a sampling frequency which means that
it should not track every span but only as often as a sampling frequency
says. This is defined in the configuration file along with all the other
options. Here is the full monitoring configuration with all its options and all
the metrics it currently is gathering. Each one of these can be turned on and
off.

.. code-block:: conf

    akka {
      cloud {
        monitoring {
          sampling-rate = 1000
          messages-per-second-sampling-rate = 1000
          max-latency-history = 1000
          max-tree-depth = 100
          allowAnnotations = on

          track-actor-type = on
          track-actor-uuid = on
          track-address = on
          track-message-type = on
          track-message-as-string = on
          track-message-send-scheme = on
          track-actor-create = on
          track-actor-start = on
          track-actor-stop = on
          track-actor-trap-exit = on
          track-actor-restart = on
          track-mailbox-size = on
          track-dispatcher = on

          track-remote-server-error = on
          track-remote-server-started = on
          track-remote-server-shutdown = on
          track-remote-server-nr-of-bytes-read = on
          track-remote-server-nr-of-bytes-written = on
          track-remote-server-client-connected = on
          track-remote-server-client-disconnected = on
          track-remote-server-client-closed = on

          track-remote-client-error = on
          track-remote-client-started = on
          track-remote-client-shutdown = on
          track-remote-client-nr-of-bytes-read = on
          track-remote-client-nr-of-bytes-written = on
          track-remote-client-connected = on
          track-remote-client-disconnected = on
        }
      }
    }


.. Preprocessed Akka JARs
.. ----------------------

.. todo:: Monitoring relies on preprocessed versions of akka-actor,
   akka-typed-actor, akka-stm and akka-remote. How to handle these from a
   installation/Maven perspective?


  Custom user-defined annotations
-------------------------------

You can add your own metadata annotation capturing application specific metrics.

First you need to enable it in the configuration.

.. code-block:: conf

    akka {
      cloud {
        monitoring {
          allowAnnotations = on
        }
      }
    }

When that is done you can use the ``Tracer`` object's ``record`` methods to add
metadata to the spans. ::

    def record(name: String)
    def record(name: String, desc: String)
    def record(name: String, desc: String, body: Array[Byte])

Here is an example::

    import akka.cloud.monitoring.Tracer

    Tracer record (
      "access-violation",
      "Unauthorized access of service [%s]".format(serviceName),
      user.getBytes)

JMX
---

All statistics data is made available through JMX. You can browse it using
JConsole that comes with the standard JDK distribution; just invoke ``jconsole``
and connect to the JMX node, or with any other JMX console. If you are using the
``LOCAL`` daemon then you have to connect to each node individually, if you are
using the ``REMOTE`` or ``FLUME`` daemons then all statistics will be consolidated
and you can connect to the node that runs the Management Server.

JMX support is turned on and off using the default ``akka.enable-jmx``
configuration option.

.. code-block:: conf

    akka {
      enable-jmx = on
    }

Here are the MBeans available:

MonitoringMBean
~~~~~~~~~~~~~~~

::

    trait MonitoringMBean {
      def start: Unit
      def stop: Unit
      def enableDaemonLocal: Unit
      def disableDaemonLocal: Unit
      def enableDaemonRemote: Unit
      def disableDaemonRemote: Unit
      def enableDaemonFlume: Unit
      def disableDaemonFlume: Unit
      def getTotalNrOfCreatedActors: Long
      def getTotalNrOfRunningActors: Long
      def getTotalNrOfActorTrapExits: Long
      def getTotalNrOfActorRestarts: Long
      def getTotalNrOfFutureTimeouts: Long
      def getTotalNrOfProcessedMessages: Long
      def getTotalNrOfBytesWritten: Long
      def getTotalNrOfBytesRead: Long
      def getTotalNrOfRemoteClientsStarted: Long
      def getTotalNrOfRemoteClientsShutdown: Long
      def getTotalNrOfRemoteClientErrors: Long
      def getTotalNrOfRemoteClientsConnected: Long
      def getTotalNrOfRemoteClientsDisconnected: Long
      def getTotalNrOfRemoteServersStarted: Long
      def getTotalNrOfRemoteServersShutdown: Long
      def getTotalNrOfRemoteServerErrors: Long
      def getTotalNrOfRemoteServerClientConnections: Long
      def getTotalNrOfRemoteServerClientDisconnections: Long
      def getTotalNrOfRemoteServerClientConnectionsClosed: Long
      def getMessagesPerSecond: Float
      def getBangsPerSecond: Float
      def getBangBangsPerSecond: Float
      def getBangBangBangsPerSecond: Float
      def getFutureResultsPerSecond: Float
      def getFutureErrorsPerSecond: Float
      def getStartTime: Long
      def getUptime: Long
      def getAvailableProcessors: Int
      def getDaemonThreadCount: Long
      def getThreadCount: Long
      def getPeakThreadCount: Long
      def getCommittedHeap: Long
      def getMaxHeap: Long
      def getUsedHeap: Long
      def getCommittedNonHeap: Long
      def getMaxNonHeap: Long
      def getUsedNonHeap: Long
    }

ActorMBean
~~~~~~~~~~

::

    trait ActorMBean {
      def getAddress: String
      def getName: String
      def getNrCreated: Long
      def getNrRunning: Long
      def getNrTrapExits: Long
      def getNrRestarts: Long
      def getNrFutureTimeouts: Long
      def getNrProcessedMessages: Long
      def getLatestMessageTimeInMailbox: Long
      def getLatestMailboxSize: Int
    }

ThreadPoolMBean
~~~~~~~~~~~~~~~

::

    trait ThreadPoolMBean {

      // monitoring
      def getIsShutdown: Boolean
      def getIsTerminated: Boolean
      def getIsTerminating: Boolean
      def getActiveCount: Long
      def getCompletedTaskCount: Long
      def getCorePoolSize: Long
      def getKeepAliveTime: Long
      def getLargestPoolSize: Long
      def getMaximumPoolSize: Long
      def getPoolSize: Long
      def getQueueSize: Long
      def getTaskCount: Long
      def getRejectionHandler: String

      // management
      def changeMaximumPoolSize(size: Int): Unit
      def changeCorePoolSize(size: Int): Unit
      def changeKeepAliveTime(millis: Long): Unit
      def changeRejectionHandlerToAbort(): Unit
      def changeRejectionHandlerToCallerRuns(): Unit
      def changeRejectionHandlerToDiscard(): Unit
      def changeRejectionHandlerToDiscardOldest(): Unit
      def clearQueue(): Unit
      def purgeCancelledTasks(): Unit
      def shutdown(): Unit
      def shutdownNow(): Unit
    }

ClusterNodeMBean
~~~~~~~~~~~~~~~~

::

    trait ClusterNodeMBean {
      def start: Unit
      def stop: Unit

      def makeAvailable: Unit
      def makeUnavailable: Unit

      def isConnected: Boolean
      def isAvailable: Boolean

      def getRemoteServerHostname: String
      def getRemoteServerPort: Int

      def getNodeName: String
      def getClusterName: String
      def getZooKeeperServerAddresses: String

      def getMemberNodes: Array[String]
      def getAvailableNodes: Array[String]
      def getLeader: String

      def getUuidsForClusteredActors: Array[String]
      def getUuidsForActorsInUse: Array[String]
      def getNodesForActorInUseWithUuid(uuid: String): Array[String]
      def getUuidsForActorsInUseOnNode(nodeName: String): Array[String]

      def setConfigElement(key: String, value: String): Unit
      def getConfigElement(key: String): AnyRef
      def removeConfigElement(key: String): Unit
      def getConfigElementKeys: Array[String]
    }

SpanMBean
~~~~~~~~~

::

    trait SpanMBean {
      def getMostRecentSpanFingerprints: Array[String]
      def getMostFrequentSpanFingerprints: Array[String]
      def getNumberOfHitsForMostFrequentSpan(fingerprint: String): Long
      def getLatestLatencyForId(fingerprint: String): Double
      def getLatencyMeanForId(fingerprint: String): Double
      def getLatencyVarianceForId(fingerprint: String): Double
      def getMaxLatencyForId(fingerprint: String): Double
      def getMinLatencyForId(fingerprint: String): Double
      def getLatenciesForId(fingerprint: String): Array[Double]
      def getFivePercentileLatency(fingerprint: String): Double
      def getTenPercentileLatency(fingerprint: String): Double
      def getFifteenPercentileLatency(fingerprint: String): Double
      def getTwentyPercentileLatency(fingerprint: String): Double
      def getTwentyFivePercentileLatency(fingerprint: String): Double
      def getThirtyPercentileLatency(fingerprint: String): Double
      def getThirtyFivePercentileLatency(fingerprint: String): Double
      def getFourtyPercentileLatency(fingerprint: String): Double
      def getFourtyFivePercentileLatency(fingerprint: String): Double
      def getFiftyPercentileLatency(fingerprint: String): Double
      def getFiftyFivePercentileLatency(fingerprint: String): Double
      def getSixtyPercentileLatency(fingerprint: String): Double
      def getSixtyFivePercentileLatency(fingerprint: String): Double
      def getSeventyPercentileLatency(fingerprint: String): Double
      def getSeventyFivePercentileLatency(fingerprint: String): Double
      def getEightyPercentileLatency(fingerprint: String): Double
      def getEightyFivePercentileLatency(fingerprint: String): Double
      def getNinetyPercentileLatency(fingerprint: String): Double
      def getNinetyFivePercentileLatency(fingerprint: String): Double
    }


SNMP
----

Coming soon.


Accessing statistics through REST API
-------------------------------------

Coming soon.


Monitoring Dashboard
--------------------

The Monitoring Dashboard is a WAR file that you can drop into any servlet
container. It comes with a servlet initializer so it will boot up itself,
including JMX and REST support (if that is enabled in the configuration).

You can also run it from SBT, which could be an easy way to get things running
while you are developing or testing things out. Simply step into the
$AKKA_CLOUD_HOME dir and run Jetty.

.. code-block:: shell

    % cd $AKKA_CLOUD_HOME
    % sbt
    > jetty-run

Now you can browse to ``http://localhost:8080`` to see the Monitoring Dashboard.

You configure the Monitoring Dashboard in the ``akka.conf`` file.

.. code-block:: conf

    akka {
      cloud {
        monitoring {
          enable-sequence-diagram-generation = on
          console-webapp-dir = "./akka-cloud-console/target/scala_2.8.1/webapp/"
          console-refresh-rate = 10
        }
      }
    }

.. todo:: Here is a screenshot


Statistics and log consolidation with REMOTE service
-------------------------------------------------

The ``REMOTE`` service is managed by the ``MonitoringServer``. If you are booting
up the Monitoring Dashboard (see section below) then the Monitoring Server is
started for you but if you want to run in "headless" mode and only access it
through JMX or REST then you can boot it up explicitly::

    import akka.cloud.monitoring._

    MonitoringServer.start

    // ...

    MonitoringServer.stop

As discussed above to enable REMOTE based monitoring service you also need to start
the ``REMOTE`` daemon locally on each node is your cluster. This is done through
the ``Monitoring`` object. ::

    // takes the config from the akka.conf file - remember to enable 'REMOTE' daemon
    Monitoring.startLocalDaemons

    // overrides the akka.conf file and starts up the 'REMOTE' daemon regardless
    Monitoring.startWithRemoteDaemon

You configure the ``REMOTE`` service in the ``akka.conf`` file.

.. code-block:: conf

    akka {
      cloud {
        monitoring {
          remote-daemon-hostname = "localhost"
          remote-daemon-port = 8700
        }
      }
    }

That's it.


Statistics and log consolidation with Flume
-------------------------------------------

Cloudy Akka Monitoring supports using `Flume
<http://archive.cloudera.com/cdh/3/flume/UserGuide.html>`_ for consolidating
statistics and logging information.

Flume is a distributed, reliable, and available service for efficiently
collecting, aggregating, and moving large amounts of log data. It has a simple
and flexible architecture based on streaming data flows. It is robust and fault
tolerant with tunable reliability mechanisms and many failover and recovery
mechanisms. The system is centrally managed and allows for intelligent dynamic
management. It uses a simple extensible data model that allows for online
analytic applications.

In order to use this service you first have to download and install Flume. See
the Flume documentation for details on how to do that.

Configuration of Flume
~~~~~~~~~~~~~~~~~~~~~~

Now you have to add the the FQN of the ``AkkaMonitoringSinkBuilder`` class to
the ``flume.plugin.classes`` option in the ``<flume-root>./conf/flume-conf.xml``
Flume configuration file.

.. code-block:: xml

    <property>
      <name>flume.plugin.classes</name>
      <value>akka.cloud.monitoring.flume.AkkaMonitoringSinkBuilder</value>
      <description>Akka plugins</description>
    </property>

Make sure you set the ``AKKA_HOME``, ``AKKA_CLOUD_HOME`` and ``SCALA_HOME``
environment variables and then set the ``FLUME_CLASSPATH`` environment variable
to this.

.. code-block:: bash

    export FLUME_CLASSPATH=\
      $AKKA_CLOUD_HOME/akka-cloud-common/target/scala_2.8.1/classes:\
      $AKKA_CLOUD_HOME/akka-cloud-monitoring/target/scala_2.8.1/classes:\
      $AKKA_CLOUD_HOME/akka-cloud-monitoring/lib_managed/scala_2.8.1/compile/configgy-2.8.0-1.5.5.jar:\
      $AKKA_CLOUD_HOME/akka-cloud-monitoring/lib_managed/scala_2.8.1/compile/akka-actor-1.0-RC2-SNAPSHOT.jar:\
      $AKKA_CLOUD_HOME/akka-cloud-monitoring/lib_managed/scala_2.8.1/compile/akka-remote-1.0-RC2-SNAPSHOT.jar:\
      $AKKA_CLOUD_HOME/akka-cloud-monitoring/lib_managed/scala_2.8.1/compile/netty-3.2.3.Final.jar:\
      $AKKA_CLOUD_HOME/akka-cloud-monitoring/lib_managed/scala_2.8.1/compile/protobuf-java-2.3.0.jar:\
      $AKKA_CLOUD_HOME/akka-cloud-monitoring/lib_managed/scala_2.8.1/compile/uuid-3.2.jar:\
      $AKKA_CLOUD_HOME/akka-cloud-monitoring/lib_managed/scala_2.8.1/compile/plantuml-1.0.jar:\
      $AKKA_CLOUD_HOME/akka-cloud-monitoring/lib_managed/scala_2.8.1/compile/commons-math-2.1.jar:\
      $AKKA_CLOUD_HOME/akka-cloud-monitoring/lib_managed/scala_2.8.1/compile/sjson-0.8-2.8.0.jar:\
      $AKKA_HOME/akka-actor/target/scala_2.8.1/classes:\
      $SCALA_HOME/lib/scala-library.jar

You configure the ``FLUME`` service in the ``akka.conf`` file. We need to add
the hostname and port to the ``MonitoringServer`` that Flume should send the
final aggregated data to. We will use the same hostname and port when we define
the Flume RPC source (``Source: rpcSource(port)``) later on when we configure
the pipeline.

.. code-block:: conf

    akka {
      cloud {
        monitoring {
          flume-rpc-hostname = "localhost"
          flume-rpc-port = 3737
        }
      }
    }

Set up a Flume workflow
~~~~~~~~~~~~~~~~~~~~~~~

Now you are all set and ready for using Flume together with the Cloudy Akka
Monitoring module.

Now let's try out a full chain:

Application -> Flume RPC Sink -> Flume RPC Source -> Akka Monitoring Sink -> Monitoring Service (over TCP) -> Monitoring Dashboard

Open three shells, let's call them Shell 1, 2 and 3.


**Shell 1**

Run Akka Console (including Akka Monitoring Service):

.. code-block:: shell

    % cd $AKKA_CLOUD_HOME
    % sbt
    > jetty-run


**Shell 2**

Run Flume Server:

.. code-block:: shell

    % cd <flume-root-dir>
    % ./bin/flume server


**Shell 3**

Run Flume Agent:

.. code-block:: shell

    % cd <flume-root-dir>
    % ./bin/flume node -n akka-node


Now browse to ``http://localhost:35871/flumeconfig.jsp`` and configure node
``akka-node``.

Here you have to define two things:

  - ``rpcSource`` -- need to be same port as the
    ``akka.cloud.monitoring.flume-rpc-port`` option in the ``akka.conf`` file.

  - ``akkaMonitoringSink`` -- needs to be host and port of the Monitoring Service
    (either started implicitly by booting up the Monitoring Dashboard or
    explicitly through the ``MonitoringServer``)

.. code-block:: conf

     Source: rpcSource(3737)
     Sink: akkaMonitoringSink(monitoringServiceHostname, monitoringServicePort)

That's it.

Now you can run your application and see the statistics flow through Flume to
the Akka Monitoring Dashboard.


Extra 1
~~~~~~~

Now we can have some more fun and add a Flume Collector to the pipeline.

E.g this chain:
Application -> Flume RPC Sink -> Flume RPC Source -> Flume Collector -> Akka Monitoring Sink -> Monitoring Service (over TCP) -> Monitoring Dashboard

**Shell 4**

Run Flume collector:

.. code-block:: shell

    % cd <flume-root-dir>
    % ./bin/flume node -n collector

Browse to ``http://localhost:35871/flumeconfig.jsp`` and configure pipeline in two steps.

Step 1: Configure node ``akka-node``:

.. code-block:: conf

    Source: rpcSource(3737)
    Sink: agentSink("localhost", 35853)

Step 2: Configure node ``collector``:

.. code-block:: conf

    Source: collectorSource(35853)
    Sink: akkaMonitoringSink("localhost", 8700)

Extra 2
~~~~~~~~

For fun you can also add syslog to the mix.

Browse to ``http://localhost:35871/flumeconfig.jsp`` and reconfigure the nodes:

Reconfigure ``akka-node``:

.. code-block:: conf

    Source: syslogTcp(5140)
    Sink: agentSink("localhost", 35853)

Reconfigure node: ``collector``:

.. code-block:: conf

    Source: collectorSource(35853)
    Sink: collectorSink("file:///tmp/flume/collected", "syslog")


.. |more| image:: more.png
          :align: middle
          :alt: More info

Flume Event Handler
-------------------

You can also use the ``FlumeEventHandler`` to log events to Flume using Akka's
regular event handler.


First you need to add it to the Akka event handlers and define the logging level::

    akka {
      event-handlers = ["akka.cloud.monitoring.flume.FlumeEventHandler"]
      event-handler-level = "DEBUG" # Options: ERROR, WARNING, INFO, DEBUG
    }

Then you also need to configure it by giving it the hostname and port of the Flume RPC service::

    akka {
      cloud {
        monitoring {
          flume-rpc-hostname = "localhost"
          flume-rpc-port = 3737
        }
      }
    }

Now we are ready to use the event handler. For all details on how to use it see the
`Akka Event Handler documentation <http://doc.akka.io/event-hander>`_, but here is
an example::

    // You can use the general 'notify' method
    EventHandler.notify(EventHandler.Error(exception, this, message.toString))

    // Or use one of the specific 'error', 'warning', 'info' or 'debug' methods
    EventHandler.error(exception, this, message.toString)

